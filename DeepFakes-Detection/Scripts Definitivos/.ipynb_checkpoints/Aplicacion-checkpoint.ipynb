{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f515d7e-1d47-4436-b5cc-da64d668a3d6",
   "metadata": {},
   "source": [
    "# Detección de Videos generados por medio de DeepFake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b185bb04-a26f-42a1-bae4-2872ddcceb17",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Importación de archivos necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b99e244-f1e7-4d69-8aa3-25c76a96213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import Video\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, make_scorer\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras_tuner as kt\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Reshape, Concatenate, LeakyReLU\n",
    "from keras.models import Model as KerasModel\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import reuters\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Conv2DTranspose\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from facenet_pytorch.models.inception_resnet_v1 import get_torch_home\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1, extract_face\n",
    "import torch\n",
    "from tensorflow.keras.applications import InceptionV3, VGG16\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f78e388a-077b-4d12-8cf6-a4e25c007f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 08:04:57.449713: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "best_model = keras.models.load_model('pretrained_vgg16_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41752f27-1193-47b1-bcc7-e59ff8240f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(images):\n",
    "    n_images = len(images)\n",
    "    f_shape = images[0].shape\n",
    "    images_p = np.empty(shape = (n_images, f_shape[-2], f_shape[-1], f_shape[-3]), dtype = int)\n",
    "    for i in range(n_images):\n",
    "        images_p[i, :, :, :] = images[i][:,:,:].permute(1,2,0).int().numpy()\n",
    "    return images_p    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "869de0d2-dba0-4b11-9b15-0a196b226f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionPipeline:\n",
    "\n",
    "    def __init__(self, detector, n_frames=50):\n",
    "\n",
    "        self.detector = detector\n",
    "        self.n_frames = n_frames\n",
    "        \n",
    "    def __call__(self, filename):\n",
    " \n",
    "   \n",
    "        v_cap = cv2.VideoCapture(filename)\n",
    "        v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "        faces = []\n",
    "        frames = []\n",
    "        def_faces = []\n",
    "        for j in range(v_len):\n",
    "            success = v_cap.grab()\n",
    "            success, frame = v_cap.retrieve()\n",
    "            if not success:\n",
    "                continue\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = Image.fromarray(frame)\n",
    "            frames.append(frame)\n",
    "        \n",
    "        idxs = [x for x in range(len(frames))]\n",
    "        cond = True\n",
    "        while cond:\n",
    "            \n",
    "            if len(idxs)>n_frames:\n",
    "                slice_idxs =  random.sample(idxs, n_frames-len(def_faces))\n",
    "                slice_idxs.sort()\n",
    "                idxs = [x for x in idxs if x not in slice_idxs]\n",
    "                sample = [frames[x] for x in slice_idxs]\n",
    "                faces = self.detector(sample)\n",
    "                temp_faces = [f for f in faces if f != None]\n",
    "                def_faces.extend(temp_faces)\n",
    "            else:\n",
    "                def_faces.extend(frames)\n",
    "\n",
    "            if len(def_faces)>=n_frames:\n",
    "                cond = False\n",
    "                def_faces = def_faces[:30]\n",
    "        \n",
    "        def_faces = process_image(def_faces)\n",
    "        v_cap.release()\n",
    "        \n",
    "        return def_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe29c2df-5e6d-4329-b908-dc3b601497d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(image_size =100 ,margin=14, factor=0.5, post_process = False).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a711266f-f3c3-4dca-bfaa-d5faab3e5614",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = 30\n",
    "detection_pipeline = DetectionPipeline(detector=mtcnn, n_frames = n_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f97aa609-dd22-48e6-89ea-46c0e5ebe00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_new_video(ruta):\n",
    "    imgs = detection_pipeline(ruta)\n",
    "    imgs = imgs.reshape(-1, 3000, 100, 3)\n",
    "    label = best_model.predict(imgs).argmax(axis=-1)\n",
    "    if label == 0:\n",
    "        pred_label = 'REAL'\n",
    "    else:\n",
    "        pred_label = 'FAKE'\n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce70861-4f11-4989-ac7b-4b6ddd0eb26d",
   "metadata": {},
   "source": [
    "# Indique la ruta del video del cual quiere predecir la etiqueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ca05d80-3498-41a2-aa2d-aec916840737",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta = 'gocqhmthdw.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a41573cd-4733-4c28-942a-14adc5426bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La etiqueta del video escogido es REAL.\n"
     ]
    }
   ],
   "source": [
    "label = predict_new_video(ruta)\n",
    "print('La etiqueta del video escogido es {}.'.format(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "612051d7-cf17-4a0f-b4ea-d9e4833a8603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"gocqhmthdw.mp4\" controls   height=\"600\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(label)\n",
    "Video(ruta, height=600)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m86",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m86"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
